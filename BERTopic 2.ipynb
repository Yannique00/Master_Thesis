{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Y.vanMegen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from nltk.corpus import stopwords\n",
    "dutch_stopwords = stopwords.words('dutch')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.graph_objects as go\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model1 = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "# embedding_model2 = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
    "# # embedding_model3 = SentenceTransformer(\"KPN/bert-base-dutch-cased\")\n",
    "# # embedding_model4 = SentenceTransformer(\"GroNLP/bert-base-dutch-cased\")\n",
    "\n",
    "# def prep_embeddings(docs):\n",
    "#     \"\"\"\n",
    "#     Prepares the embeddings for the documents using the specified embedding model.\n",
    "#     \"\"\"\n",
    "#     embeddings1 = embedding_model1.encode(docs, show_progress_bar=True)\n",
    "#     embeddings2 = embedding_model2.encode(docs, show_progress_bar=True)\n",
    "\n",
    "#     return [embeddings1, embeddings2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(n_models, embeddings):\n",
    "    \"\"\"\n",
    "    Get the embedding model.\n",
    "    \"\"\"\n",
    "    if n_models == 1:\n",
    "        embedding_model = embedding_model1\n",
    "        embeddings = embeddings\n",
    "    elif n_models == 2:\n",
    "        embedding_model = embedding_model2\n",
    "        embeddings = embeddings\n",
    "    # elif n_models == 3:\n",
    "    #     embedding_model = embedding_model3\n",
    "    # elif n_models == 4:\n",
    "    #     embedding_model = embedding_model4\n",
    "    else:\n",
    "        raise ValueError(\"Invalid number of models.\")\n",
    "    return embedding_model, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bertopic_model(n_topics, min_topic_size=10):\n",
    "    # embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    # embedding_model, embeddings = get_embedding(n_models)\n",
    "    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_topic_size, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "    vectorizer_model = CountVectorizer(stop_words=dutch_stopwords)\n",
    "    # vectorizer_model = CountVectorizer(stop_words='english')\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        nr_topics=n_topics,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence_score(texts, topic_words):\n",
    "    # Convert each document to a list if it's a string representation of a list\n",
    "    texts = [ast.literal_eval(doc) if isinstance(doc, str) else doc for doc in texts]\n",
    "    \n",
    "    dictionary = Dictionary(texts)\n",
    "\n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=topic_words,  \n",
    "        texts=texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "\n",
    "    return coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_topics_with_sentiment(texts, embeddings, n_topics_list=[5, 10, 15], n_words_list=[5, 10, 15]):\n",
    "def analyze_topics_with_sentiment(texts, embeddings, n_topics_list=[5], n_words_list=[5]):\n",
    "    results = {}\n",
    "\n",
    "    # n_models_list = [1, 2]\n",
    "    \n",
    "    # to create figure for coherence scores across different configurations\n",
    "    coherence_fig = go.Figure()\n",
    "    \n",
    "    for n_topics in n_topics_list:\n",
    "        coherence_scores = []\n",
    "        for n_words in n_words_list:\n",
    "            # for n_models in n_models_list:\n",
    "                # print(f\"\\nAnalyzing with {n_topics} topics and {n_words} words per topic and embedding model {n_models}...\")\n",
    "                print(f\"\\nAnalyzing with {n_topics} topics and {n_words} words per topic...\")\n",
    "                \n",
    "                # if n_models == 1:\n",
    "                #     embeddings = embedding1\n",
    "                # elif n_models == 2:\n",
    "                #     embeddings = embeddings2\n",
    "\n",
    "                topic_model = create_bertopic_model(n_topics)\n",
    "                \n",
    "                topics, probs = topic_model.fit_transform(texts, embeddings)\n",
    "                \n",
    "                topic_info = topic_model.get_topic_info()\n",
    "                \n",
    "                # Get topics with specified number of words\n",
    "                topic_words = {}\n",
    "                for topic in range(-1, len(set(topics))-1):\n",
    "                    words = topic_model.get_topic(topic)[:n_words]\n",
    "                    topic_words[topic] = [word for word, _ in words]\n",
    "            \n",
    "\n",
    "                for topic in range(-1, len(set(topics)) - 1):\n",
    "                    print(f\"Topic {topic}: {topic_model.get_topic(topic)}\")\n",
    "\n",
    "                # Get topics with specified number of words (words only, no probabilities)\n",
    "                topic_word_list = []\n",
    "                for topic in range(len(set(topics)) - 1):\n",
    "                    topic_words = [word for word, _ in topic_model.get_topic(topic)[:n_words]]\n",
    "                    topic_word_list.append(topic_words)\n",
    "\n",
    "\n",
    "                # Coherence score\n",
    "                coherence = calculate_coherence_score(texts, topic_word_list)\n",
    "                coherence_scores.append(coherence)\n",
    "                \n",
    "                # # Topic-sentiment relationship to see how much the topics relate to a positive stock result\n",
    "                # topic_sentiment = {}\n",
    "                # for topic_num in range(-1, len(set(topics))-1):\n",
    "                #     topic_docs = [i for i, t in enumerate(topics) if t == topic_num]\n",
    "                #     if topic_docs:\n",
    "                #         topic_targets = [targets[i] for i in topic_docs]\n",
    "                #         positive_ratio = sum(topic_targets) / len(topic_targets)\n",
    "                #         topic_sentiment[topic_num] = positive_ratio\n",
    "                \n",
    "                results[(n_topics, n_words)] = {\n",
    "                    'model': topic_model,\n",
    "                    'topics': topics,\n",
    "                    'topic_info': topic_info,\n",
    "                    'topic_words': topic_words,\n",
    "                    # 'topic_sentiment': topic_sentiment,\n",
    "                    'coherence': coherence\n",
    "                }\n",
    "                \n",
    "                # to save the visualizations\n",
    "                fig_topics = topic_model.visualize_topics()\n",
    "                fig_topics.write_html(f'topic_visualization_{n_topics}_{n_words}.html')\n",
    "                \n",
    "                fig_heatmap = topic_model.visualize_heatmap()\n",
    "                fig_heatmap.write_html(f'topic_heatmap_{n_topics}_{n_words}.html')\n",
    "                \n",
    "                # # Topic-sentiment visualization\n",
    "                # sentiment_data = pd.DataFrame.from_dict(topic_sentiment, orient='index', columns=['positive_ratio'])\n",
    "                # fig_sentiment = go.Figure(data=[\n",
    "                #     go.Bar(x=sentiment_data.index, y=sentiment_data['positive_ratio'])\n",
    "                # ])\n",
    "                # fig_sentiment.update_layout(\n",
    "                #     title=f'Topic-Sentiment Relationship ({n_topics} topics)',\n",
    "                #     xaxis_title='Topic Number',\n",
    "                #     yaxis_title='Ratio of Positive Stock Movement',\n",
    "                #     yaxis_range=[0, 1]\n",
    "                # )\n",
    "                # fig_sentiment.write_html(f'topic_sentiment_{n_topics}_{n_words}.html')\n",
    "            \n",
    "        # Add coherence scores to the plot\n",
    "        coherence_fig.add_trace(go.Scatter(\n",
    "            x=n_words_list,\n",
    "            y=coherence_scores,\n",
    "            mode='lines+markers',\n",
    "            name=f'{n_topics} topics'\n",
    "        ))\n",
    "    \n",
    "    coherence_fig.update_layout(\n",
    "        title='Coherence Scores across Different Configurations',\n",
    "        xaxis_title='Number of Words per Topic',\n",
    "        yaxis_title='Coherence Score (C_v)',\n",
    "        showlegend=True\n",
    "    )\n",
    "    coherence_fig.write_html('coherence_scores.html')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_results_summary(results):\n",
    "#     for (n_topics, n_words), result in results.items():\n",
    "#         print(f\"\\n=== Results for {n_topics} topics with {n_words} words ===\")\n",
    "#         print(f\"Coherence Score: {result['coherence']:.4f}\")\n",
    "        \n",
    "#         # Print topics and their words\n",
    "#         print(\"\\nTopics and their key words:\")\n",
    "#         print(\"results:\", result)\n",
    "\n",
    "#         topic_info = result['topic_info']\n",
    "#         for index, row in topic_info.iterrows():\n",
    "#             topic_num = row['Topic']\n",
    "#             if topic_num != -1:  # skip outlier topic\n",
    "#                 words = row['Representation']\n",
    "#                 print(f\"Topic {topic_num}: {words}\")\n",
    "\n",
    "\n",
    "        \n",
    "#         # If `result['topic_words']` is a list, we iterate directly over it\n",
    "#         for topic_num, words in enumerate(result['topic_words']):\n",
    "#             print('topic_num:', topic_num)\n",
    "#             print('words:', words)\n",
    "#             if topic_num != -1:  \n",
    "#                 print(f\"Topic {topic_num}: {words}\")\n",
    "        \n",
    "       \n",
    "#         # Print topic sizes\n",
    "#         topic_sizes = result['topic_info']['Count'].tolist()\n",
    "#         print(\"\\nTopic sizes:\", topic_sizes)\n",
    "        \n",
    "#         print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "def print_results_summary(results, texts):\n",
    "    for (n_topics, n_words), result in results.items():\n",
    "        print(f\"\\n=== Results for {n_topics} topics with {n_words} words ===\")\n",
    "        print(f\"Coherence Score: {result['coherence']:.4f}\")\n",
    "        \n",
    "        print(\"\\nTopics and their key words:\")\n",
    "        topic_info = result['topic_info']\n",
    "        for _, row in topic_info.iterrows():\n",
    "            topic_num = row['Topic']\n",
    "            if topic_num != -1:\n",
    "                words = row['Representation']\n",
    "                print(f\"Topic {topic_num}: {', '.join(words)}\")\n",
    "\n",
    "        topic_sizes = topic_info[topic_info['Topic'] != -1]['Count'].tolist()\n",
    "        print(\"\\nTopic sizes:\", topic_sizes)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    \n",
    "        # Print per-document topic assignment and top words\n",
    "        model = result['model']\n",
    "        topics = result['topics']\n",
    "        docs = texts\n",
    "\n",
    "        print(\"\\nTop topic keywords per document:\")\n",
    "        for doc, topic in zip(docs, topics):\n",
    "            if topic != -1:\n",
    "                topic_words = model.get_topic(topic)\n",
    "                top_words = \", \".join([word for word, _ in topic_words[:5]])  # Top 5 words\n",
    "                print(f\"Document: {doc[:100]}...\")  # preview first 100 chars\n",
    "                print(f\"Assigned Topic: {topic}\")\n",
    "                print(f\"Topic Keywords: {top_words}\")\n",
    "                print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data = pd.read_csv('a:/df_cleaned.csv')\n",
    "\n",
    "\n",
    "    remove_list = ['mg', 'x', 'per', 'dag', 'samenvatting', 'beleid', 'conclusie', 'mmolL', 'waarvoor', 'goed', 'wel', 'beloop', \n",
    "            'voorgeschiedenis', 'opdrachten', 'gehad', 'aanvullend', 'bekende', 'voltooid', 'mogelijk', 'gezien', 'city', 'bsn', \n",
    "            'nodig', 'firstname', 'streetname', 'lastname', 'postcode', 'anamnese',\n",
    "            'dd', 'stuk', 'ivm', 'rechts', 'links', 'dr', 'sinds', 'huisarts', 'datum', 'dagen', 'min', 'extra', 'weken', 'algemeen', \n",
    "            'patiënte', 'overige','linker', 'week', 'accepteren', 'maanden', 'waarschijnlijk', 'reden', 'uur', 'verdenking', 'ontslag', \n",
    "            'stop', 'tijd', 'patiënt', 'onderzoek']\n",
    "\n",
    "    data['tokens'] = data['tokens']#.apply(lambda tokens: ''.join(tokens))\n",
    "    print(\"tokens:\", data['tokens'])\n",
    "    \n",
    "        \n",
    "    # Convert the tokens column (which is a list of words) into a single string for each document\n",
    "    data['text'] = data['tokens'].apply(lambda tokens: [word for word in tokens if word not in remove_list])\n",
    "    print(\"text:\", data['text'])\n",
    "    \n",
    "#     data['tokens'] = data['tokens'].apply(ast.literal_eval)\n",
    "    \n",
    "    texts = data['text'].tolist() \n",
    "    print(\"texts:\", texts)\n",
    "\n",
    "    # embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    # # embedding_model2 = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
    "    # embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "    # # embeddings2 = embedding_model2.encode(docs, show_progress_bar=True)\n",
    "\n",
    "    import numpy as np\n",
    "    # # Save the embeddings to a file\n",
    "    # np.save('embeddings_model1.npy', embeddings)\n",
    "    # # Load the embeddings from the file\n",
    "    embeddings = np.load('embeddings_model1.npy')\n",
    "    print(\"embeddings:\", embeddings)    \n",
    "\n",
    "    # targets = merged_df['target'].tolist()\n",
    "\n",
    "\n",
    "    # for i in range(len(data['tokens'])):\n",
    "    #     data['tokens'][i] = f\"{data['tokens'][i]}\"\n",
    "\n",
    "    # Analyze with different numbers of topics and words\n",
    "    results = analyze_topics_with_sentiment(texts, embeddings)\n",
    "    \n",
    "    # Print detailed results summary\n",
    "    print_results_summary(results, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 0       ['dhr', 'aj', 'dingemans', 'huisarts', 'street...\n",
      "1       ['samenvatting', 'rectaal', 'bloedverlie', 'ob...\n",
      "2       ['coloscopie', 'betreffen', 'mw', 'initials', ...\n",
      "3       ['gastroscopie', 'betreffen', 'mw', 'initials'...\n",
      "4       ['samenvatting', 'rectaal', 'bloedverlie', 'ee...\n",
      "                              ...                        \n",
      "9572    ['samenvatting', 'consult', 'type', 'consult',...\n",
      "9573    ['samenvatting', 'decursus', 'type', 'decursus...\n",
      "9574    ['samenvatting', 'verpleegkundig', 'verslagleg...\n",
      "9575    ['samenvatting', 'medisch', 'dossier', 'vk', '...\n",
      "9576    ['samenvatting', 'verpleegkundig', 'verslagleg...\n",
      "Name: tokens, Length: 9577, dtype: object\n",
      "text: 0       [[, ', d, h, r, ', ,,  , ', a, j, ', ,,  , ', ...\n",
      "1       [[, ', s, a, m, e, n, v, a, t, t, i, n, g, ', ...\n",
      "2       [[, ', c, o, l, o, s, c, o, p, i, e, ', ,,  , ...\n",
      "3       [[, ', g, a, s, t, r, o, s, c, o, p, i, e, ', ...\n",
      "4       [[, ', s, a, m, e, n, v, a, t, t, i, n, g, ', ...\n",
      "                              ...                        \n",
      "9572    [[, ', s, a, m, e, n, v, a, t, t, i, n, g, ', ...\n",
      "9573    [[, ', s, a, m, e, n, v, a, t, t, i, n, g, ', ...\n",
      "9574    [[, ', s, a, m, e, n, v, a, t, t, i, n, g, ', ...\n",
      "9575    [[, ', s, a, m, e, n, v, a, t, t, i, n, g, ', ...\n",
      "9576    [[, ', s, a, m, e, n, v, a, t, t, i, n, g, ', ...\n",
      "Name: text, Length: 9577, dtype: object\n",
      "texts: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ['dhr', 'aj', 'dingemans', 'huisarts', 'street...\n",
      "1    ['samenvatting', 'rectaal', 'bloedverlie', 'ob...\n",
      "2    ['coloscopie', 'betreffen', 'mw', 'initials', ...\n",
      "3    ['gastroscopie', 'betreffen', 'mw', 'initials'...\n",
      "4    ['samenvatting', 'rectaal', 'bloedverlie', 'ee...\n",
      "Name: text, dtype: object\n",
      "0    [dhr, aj, dingemans, streetnaam, Kenmerk, pati...\n",
      "1    [rectaal, bloedverlie, obvn, divertikelbloedin...\n",
      "2    [coloscopie, betreffen, mw, initials, adresgeg...\n",
      "3    [gastroscopie, betreffen, mw, initials, adresg...\n",
      "4    [rectaal, bloedverlie, eenmalig, hd, hbstabiel...\n",
      "Name: tokens, dtype: object\n",
      "0    dhr aj dingemans streetnaam Kenmerk patientid ...\n",
      "1    rectaal bloedverlie obvn divertikelbloeding ac...\n",
      "2    coloscopie betreffen mw initials adresgegevens...\n",
      "3    gastroscopie betreffen mw initials adresgegeve...\n",
      "4    rectaal bloedverlie eenmalig hd hbstabiel inr ...\n",
      "Name: text, dtype: object\n",
      "['dhr aj dingemans streetnaam Kenmerk patientid betreffen mevrouw initials geb birthdate streetnaam zip tel phonenumber geacht collega bovengenoemde opnemen afdeling maag darm leverziekt verband melaena rectaal bloedverlie diep veneuaz trombose longembolie cholecystectomie diverticulitis atriumfibrilleren spontaan conversie sinusritme melena verklaring vinden verband stabiel hb overleg expectatief vermoeidheid sinusbradycardie metoprolol tambocor anamnees vanmiddag fors Helderrood bloedverlie stolsel vermengen ontlasting zwart kleur zeuren pijn bovenbuik maagpijn stoppen koffie drinken vet eten ontlasting intaak bloed zwart verkleuring bemerken tractus anamnees bijdragen mn lwklachten all penicilline urticaria lichamelijk controle hr bpm nibp mmhg temp alg acuut ziek duidelijk anemisch hh pearl lymfadenopathie Cor souffle pulm vag beiderzijds bijgeluiod abd normaal peristaltiek wisselen tympanie soepel abdomen mild drukpijn epigastrio loslaatpijn murphy rt Helderrood bloed handschoen feces afwijking palpabel hemorroïden aanvullen laboratorium hematologie crp mgl hemoglobine Mmoll hematocriet ll erytrocyte mcv fl leukocyt trombocyt hemostase aptt sec pt sec inr bloedgroep bloedgroep neg iat neg negatief chemie glucose mmoll natrium Mmoll kalium Mmoll ureum mmoll kreatinine µmoll Egfr ckdepi bilirubine totaal µmoll alkalisch fosfatase ul ggt ul asat ul alat ul ld ul lipase ul albumine gl gastroscopie bloed bloedingsbron gastroscopie coloscopie coloscopie colon ascendens lang darm scopie klep bloeding stoppen bloedingsbron vinden pandiverticulose Sigmoid waarschijnlijkheidsdiagnose divertikelbloeding laboratorium hematologie hemoglobine Mmoll mcv fl opnemen verband melaena hierbij Helderrood rectaal bloedverlie acenocoumarol acenocoumarol staken couperen zowel hoog laag tractus digestivus bloeding starten pantoprazol perfusor voorbereiden zowel gastro coloscopie avond nogmaals fors melaena hypotensief rr infuus starten hierbij klacht volgen krijgen zowel gasto coloscopie waarop uitbreiden diverticulose zien actief bloedingsbron waardoor werkdiagnose divertikelbloeding coloscopie coecumbodem volledig beoordeeld familie bespreken theoretisch coecumproce zitten kans klein caecum redelijk overzien hiervoor ctcolografie maken samenspraak familie hiervan afzien endoscopie rectaal bloedverlie waarnemen conditie huis medicatie latanoprosten oogdruppel minim oog druppel oog hyaluronzuur oogdruppel fl oog druppel acenocoumarol tablet oraal trombosedienst macrogolzout pdr drank movicolongeneriek oraal furosemide tablet oraal tijdelijk stoppen metocloprami tablet oraal behandelbeperking kiezen behandelbeperking reanimeren nee ic opname beslissen beademen nee divertikelbloeding acenocoumarol opnaam meermaals hypotensie rr klacht acenocoumarol dosering hervatten furosemide tijdelijk staken laag tensie meet bloeddruk thuis overnieuw nemen hierover contact vriendelijk groet bukkem semiarts maagdarmleverziekt vriendelijk groet jt kamphuis maagdarmleverarts agb brief elektronisch accorderen ondertekenen', 'rectaal bloedverlie obvn divertikelbloeding acenocoumarol couperen colo pandiverticulose vpk rectaal bloedverlie voelen klacht laag tensie graag huis eten vanochtend hierbij klacht bed klacht gevoel huis gaan bloed zien schoondochter uitslag coloscopie bespreken aangeven coecum bodem volledig beeld brengen grodendeel caecumproce volledig uitsluiten pte ongerust ctcolografie laten pte schoondochter ongerust hoeven aangeven bedenken terug terugbellen lichamelijk hd stabiel koorts rr laag kant Furosemide bloedverlie waarnemen klinisch vandaag hypotensie furosemien adviseren tensie thuis opvolgen iom kamphuis acenocoumarol dosering hervatten uitleg zoon pte coecum bodem brief tensiecontrole vermelden', 'coloscopie betreffen mw initials adresgegevens streetnaam zip geboortedatum birthdate asa classificatie verwijzer jt kamphuis indicatie bloedverliesrectaal melena bloedverlie anum acenocoumarol afwijkingen top verrichten informed consent endoscoop geacht collega coloscopie uitvoeren indicatie bloedverliesrectaal melena bloedverlie anum acenocoumarol afwijkingen toegedien medicatie midazolam Fentanyl Fentanyl introductie intubatie plaatsvinden colon ascendens opvoeren lukken uitbocht endoscoop boston bowel preparation score terugtrektijd patiëntcomfort mild ongemak procedure twee voorvalal ongemak voldoende verdragen voorbereiding laxeren nuchter verslag rt afwijking introductie lang bochtig darm klep ondanks buikcompressie verschillend houding introductie volledig scoop lukken caecumbodem bereiken nergens bloed bloedingsbron pandiverticulose Sigmoid poliep tumor inflammatie rectum inversie afwijking coloscopie colon ascendens lang darm scopie klep bloeding stoppen bloedingsbron vinden waarschijnlijkheidsdiagnose divertikelbloeding advies gegeven gastro coloscopie waarschijnlijkheidsdiagnose divertikelbloeding uitsluiten caecumproce mbt ctcolografie kans klein afwijking caecum grotendeels overzien bespreken poliklinisch', 'gastroscopie betreffen mw initials adresgegevens streetnaam zip geboortedatum birthdate asa classificatie verwijzer jt kamphuis indicatie Melena bloedverlie anum acenocoumarol afwijkingen top verrichten informed consent endoscoop geacht collega gastroscopie uitvoeren indicatie Melena bloedverlie anum acenocoumarol afwijkingen toegedien medicatie midazolam Fentanyl voorbereiding nuchter verslag slokdarm zlijn diafragma cm oesofagitis barrett maag atrofisch rood aspect slijmvlie echt gastritis ulcera normaal inversie duodenum dii normaal vlokken plooienpatroon ulcera groot duodenum divertikel nergens bloed bloed bloedingsbron gastroscopie', 'rectaal bloedverlie eenmalig hd hbstabiel inr wd divertikelbloeding maagulcus zien pijn epigastrio zwart ontlasting ureum acenocoumarol coupeeren vpk gisteravond melena vannacht donkerrood bloedverlie stolsel gisteravond pijnklacht maag pijnstilling hiervoor voelen infuus bed voelen gisteren plots erg slecht denken doodging opgestaan keuken wandeld omgevallen zeggen bewustzijn verliezen grond vallen familie bellen meteen komen wc liggen vol bloed gaan buikpijnklacht wc waarbij bloed zitten lichamelijk hd stabiel vannacht tensiedaling wv infuus koorts aanvullen lab hemoglobine Mmoll mcv fl inr gastroscopie bloed bloedingsbron gastroscopie coloscopie coloscopie colon ascendens lang darm scopie klep bloeding stoppen bloedingsbron vinden waarschijnlijkheidsdiagnose divertikelbloeding pandiverticulose rectaal bloedverlie obvn divertikelbloeding iom kamphuis gastrocolo vandaag morgen hb controle uitslag vertellen pte zoon opdracht verpleegkundig vod voltooien ews voltooien']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('a:/df_cleaned.csv')\n",
    "\n",
    "# # Adjustments for word removal\n",
    "# merged_df['tokens'] = merged_df['tokens'].apply(ast.literal_eval)\n",
    "\n",
    "# print(data.head())\n",
    "# print(data.columns)\n",
    "\n",
    "    \n",
    "# Convert the tokens column (which is a list of words) into a single string for each document\n",
    "data['text'] = data['tokens'].apply(lambda tokens: ''.join(tokens))    \n",
    "print(data['text'].head())\n",
    "\n",
    "data['tokens'] = data['tokens'].apply(ast.literal_eval)\n",
    "\n",
    "texts = data['text'].tolist() \n",
    "\n",
    "remove_list = ['mg', 'x', 'per', 'dag', 'samenvatting', 'beleid', 'conclusie', 'mmolL', 'waarvoor', 'goed', 'wel', 'beloop', \n",
    "            'voorgeschiedenis', 'opdrachten', 'gehad', 'aanvullend', 'bekende', 'voltooid', 'mogelijk', 'gezien', 'city', 'bsn', \n",
    "            'nodig', 'firstname', 'streetname', 'lastname', 'postcode', 'anamnese',\n",
    "            'dd', 'stuk', 'ivm', 'rechts', 'links', 'dr', 'sinds', 'huisarts', 'datum', 'dagen', 'min', 'extra', 'weken', 'algemeen', \n",
    "            'patiënte', 'overige','linker', 'week', 'accepteren', 'maanden', 'waarschijnlijk', 'reden', 'uur', 'verdenking', 'ontslag', \n",
    "            'stop', 'tijd', 'patiënt', 'onderzoek']\n",
    "\n",
    "data['tokens'] = data['tokens'].apply(lambda tokens: [word for word in tokens if word not in remove_list])\n",
    "\n",
    "data['text'] = data['tokens'].apply(lambda tokens: ' '.join(tokens))   \n",
    "texts = data['text'].tolist() \n",
    "print(data['tokens'].head())\n",
    "print(data['text'].head())\n",
    "print(texts[:5])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('a:/df_cleaned.csv')\n",
    "\n",
    "# # # Adjustments for word removal\n",
    "# # merged_df['tokens'] = merged_df['tokens'].apply(ast.literal_eval)\n",
    "\n",
    "# # print(data.head())\n",
    "# # print(data.columns)\n",
    "\n",
    "    \n",
    "# # Convert the tokens column (which is a list of words) into a single string for each document\n",
    "# remove_list = ['mg', 'x', 'per', 'dag', 'samenvatting', 'beleid', 'conclusie', 'mmolL', 'waarvoor', 'goed', 'wel', 'beloop', \n",
    "#             'voorgeschiedenis', 'opdrachten', 'gehad', 'aanvullend', 'bekende', 'voltooid', 'mogelijk', 'gezien', 'city', 'bsn', \n",
    "#             'nodig', 'firstname', 'streetname', 'lastname', 'postcode', 'anamnese',\n",
    "#             'dd', 'stuk', 'ivm', 'rechts', 'links', 'dr', 'sinds', 'huisarts', 'datum', 'dagen', 'min', 'extra', 'weken', 'algemeen', \n",
    "#             'patiënte', 'overige','linker', 'week', 'accepteren', 'maanden', 'waarschijnlijk', 'reden', 'uur', 'verdenking', 'ontslag', \n",
    "#             'stop', 'tijd', 'patiënt', 'onderzoek']\n",
    "\n",
    "# data['tokens'] = data['tokens'].apply(lambda tokens: [word for word in tokens if word not in remove_list])\n",
    "\n",
    "# data['text'] = data['tokens'].apply(lambda tokens: ' '.join(tokens))   \n",
    "# texts = data['text'].tolist() \n",
    "\n",
    "# embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "# # embedding_model2 = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
    "# embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "\n",
    "# # Save the embeddings to a file\n",
    "# np.save('embeddings_model1.npy', embeddings)\n",
    "# # # Load the embeddings from the file\n",
    "# embeddings = np.load('embeddings_model1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# # Save the embeddings to a file\n",
    "# np.save('embeddings_model1.npy', embeddings)\n",
    "# # # Load the embeddings from the file\n",
    "# embeddings = np.load('embeddings_model1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
