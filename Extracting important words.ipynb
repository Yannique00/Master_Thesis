{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pseudo_id</th>\n",
       "      <th>verslagen_report_tags</th>\n",
       "      <th>verslagen_report_content</th>\n",
       "      <th>verslagen_report_start_date</th>\n",
       "      <th>verslagen_report_content_cleaned</th>\n",
       "      <th>content_words</th>\n",
       "      <th>content_words_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6</td>\n",
       "      <td>Klinische Brief</td>\n",
       "      <td>Dhr. A.J. Dingemans, huisarts\\r\\n[STREETNAME] ...</td>\n",
       "      <td>2020-11-26 15:06:00</td>\n",
       "      <td>dhr aj dingemans huisarts streetname nr city d...</td>\n",
       "      <td>['dhr', 'aj', 'dingemans', 'huisarts', 'street...</td>\n",
       "      <td>['dhr', 'aj', 'dingemans', 'huisarts', 'street...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6</td>\n",
       "      <td>Consult, Kliniek: vervolgconsult</td>\n",
       "      <td>Samenvatting: \\nRectaal bloedverlies obv diver...</td>\n",
       "      <td>2020-11-26 09:53:00</td>\n",
       "      <td>samenvatting rectaal bloedverlies obv divertik...</td>\n",
       "      <td>['samenvatting', 'rectaal', 'bloedverlies', 'o...</td>\n",
       "      <td>['samenvatting', 'rectaal', 'bloedverlie', 'ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6</td>\n",
       "      <td>Poliklinische Brief</td>\n",
       "      <td>COLOSCOPIE\\r\\n\\r\\nBetreft\\r\\nMw. [INITIALS] [L...</td>\n",
       "      <td>2020-11-25 14:13:00</td>\n",
       "      <td>coloscopie betreft mw initials lastname adresg...</td>\n",
       "      <td>['coloscopie', 'betreft', 'mw', 'initials', 'l...</td>\n",
       "      <td>['coloscopie', 'betreffen', 'mw', 'initials', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6</td>\n",
       "      <td>Poliklinische Brief</td>\n",
       "      <td>GASTROSCOPIE\\r\\n\\r\\nBetreft\\r\\nMw. [INITIALS] ...</td>\n",
       "      <td>2020-11-25 13:48:00</td>\n",
       "      <td>gastroscopie betreft mw initials lastname adre...</td>\n",
       "      <td>['gastroscopie', 'betreft', 'mw', 'initials', ...</td>\n",
       "      <td>['gastroscopie', 'betreffen', 'mw', 'initials'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6</td>\n",
       "      <td>Consult, Kliniek: vervolgconsult</td>\n",
       "      <td>Samenvatting: \\nRectaal bloedverlies ; eenmali...</td>\n",
       "      <td>2020-11-25 08:47:00</td>\n",
       "      <td>samenvatting rectaal bloedverlies eenmalig hd ...</td>\n",
       "      <td>['samenvatting', 'rectaal', 'bloedverlies', 'e...</td>\n",
       "      <td>['samenvatting', 'rectaal', 'bloedverlie', 'ee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  pseudo_id             verslagen_report_tags  \\\n",
       "0  046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6                   Klinische Brief   \n",
       "1  046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6  Consult, Kliniek: vervolgconsult   \n",
       "2  046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6               Poliklinische Brief   \n",
       "3  046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6               Poliklinische Brief   \n",
       "4  046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6  Consult, Kliniek: vervolgconsult   \n",
       "\n",
       "                            verslagen_report_content  \\\n",
       "0  Dhr. A.J. Dingemans, huisarts\\r\\n[STREETNAME] ...   \n",
       "1  Samenvatting: \\nRectaal bloedverlies obv diver...   \n",
       "2  COLOSCOPIE\\r\\n\\r\\nBetreft\\r\\nMw. [INITIALS] [L...   \n",
       "3  GASTROSCOPIE\\r\\n\\r\\nBetreft\\r\\nMw. [INITIALS] ...   \n",
       "4  Samenvatting: \\nRectaal bloedverlies ; eenmali...   \n",
       "\n",
       "  verslagen_report_start_date  \\\n",
       "0         2020-11-26 15:06:00   \n",
       "1         2020-11-26 09:53:00   \n",
       "2         2020-11-25 14:13:00   \n",
       "3         2020-11-25 13:48:00   \n",
       "4         2020-11-25 08:47:00   \n",
       "\n",
       "                    verslagen_report_content_cleaned  \\\n",
       "0  dhr aj dingemans huisarts streetname nr city d...   \n",
       "1  samenvatting rectaal bloedverlies obv divertik...   \n",
       "2  coloscopie betreft mw initials lastname adresg...   \n",
       "3  gastroscopie betreft mw initials lastname adre...   \n",
       "4  samenvatting rectaal bloedverlies eenmalig hd ...   \n",
       "\n",
       "                                       content_words  \\\n",
       "0  ['dhr', 'aj', 'dingemans', 'huisarts', 'street...   \n",
       "1  ['samenvatting', 'rectaal', 'bloedverlies', 'o...   \n",
       "2  ['coloscopie', 'betreft', 'mw', 'initials', 'l...   \n",
       "3  ['gastroscopie', 'betreft', 'mw', 'initials', ...   \n",
       "4  ['samenvatting', 'rectaal', 'bloedverlies', 'e...   \n",
       "\n",
       "                            content_words_lemmatized  \n",
       "0  ['dhr', 'aj', 'dingemans', 'huisarts', 'street...  \n",
       "1  ['samenvatting', 'rectaal', 'bloedverlie', 'ob...  \n",
       "2  ['coloscopie', 'betreffen', 'mw', 'initials', ...  \n",
       "3  ['gastroscopie', 'betreffen', 'mw', 'initials'...  \n",
       "4  ['samenvatting', 'rectaal', 'bloedverlie', 'ee...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_verslagen_clean = pd.read_csv('a:/df_verslagen_cleaned.csv')\n",
    "df_verslagen_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at CLTL/MedRoBERTa.nl and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LABEL_0', 'score': np.float32(0.67044556), 'index': 1, 'word': 'De', 'start': 0, 'end': 2}, {'entity': 'LABEL_1', 'score': np.float32(0.5582332), 'index': 2, 'word': 'ĠpatiÃ«nt', 'start': 3, 'end': 10}, {'entity': 'LABEL_1', 'score': np.float32(0.5922579), 'index': 3, 'word': 'Ġheeft', 'start': 11, 'end': 16}, {'entity': 'LABEL_1', 'score': np.float32(0.7423673), 'index': 4, 'word': 'Ġdiabetes', 'start': 17, 'end': 25}, {'entity': 'LABEL_1', 'score': np.float32(0.6422724), 'index': 5, 'word': 'Ġtype', 'start': 26, 'end': 30}, {'entity': 'LABEL_0', 'score': np.float32(0.60533726), 'index': 6, 'word': 'Ġ2', 'start': 31, 'end': 32}, {'entity': 'LABEL_1', 'score': np.float32(0.65782994), 'index': 7, 'word': 'Ġen', 'start': 33, 'end': 35}, {'entity': 'LABEL_1', 'score': np.float32(0.56345296), 'index': 8, 'word': 'Ġkreeg', 'start': 36, 'end': 41}, {'entity': 'LABEL_1', 'score': np.float32(0.60257363), 'index': 9, 'word': 'Ġeen', 'start': 42, 'end': 45}, {'entity': 'LABEL_1', 'score': np.float32(0.5523825), 'index': 10, 'word': 'ĠMRI', 'start': 46, 'end': 49}, {'entity': 'LABEL_0', 'score': np.float32(0.72472394), 'index': 11, 'word': '-', 'start': 49, 'end': 50}, {'entity': 'LABEL_0', 'score': np.float32(0.69249195), 'index': 12, 'word': 'scan', 'start': 50, 'end': 54}, {'entity': 'LABEL_1', 'score': np.float32(0.64037824), 'index': 13, 'word': 'Ġvoor', 'start': 55, 'end': 59}, {'entity': 'LABEL_0', 'score': np.float32(0.678544), 'index': 14, 'word': 'Ġknieklachten', 'start': 60, 'end': 72}, {'entity': 'LABEL_0', 'score': np.float32(0.6106078), 'index': 15, 'word': '.', 'start': 72, 'end': 73}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"CLTL/MedRoBERTa.nl\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create a pipeline for entity recognition\n",
    "nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example Dutch medical note\n",
    "text = \"De patiënt heeft diabetes type 2 en kreeg een MRI-scan voor knieklachten.\"\n",
    "\n",
    "# Run the model\n",
    "entities = nlp_ner(text)\n",
    "print(entities)  # This will highlight key medical terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key medical terms: ['type', '2', 'diabetes', 'mri', 'knee']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pretrained medical NER model\n",
    "nlp_ner = pipeline(\"ner\", model=\"d4data/biomedical-ner-all\", tokenizer=\"d4data/biomedical-ner-all\")\n",
    "\n",
    "# Example English medical note (translate Dutch input to English first)\n",
    "text = \"Patient has type 2 diabetes and received an MRI scan for knee pain.\"\n",
    "\n",
    "# Run NER model\n",
    "entities = nlp_ner(text)\n",
    "\n",
    "# Extract key terms\n",
    "keywords = [entity[\"word\"] for entity in entities]\n",
    "print(\"Key medical terms:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 26, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"FremyCompany/BioLORD-2023\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)  # Change from AutoModelForSeq2SeqLM to AutoModel\n",
    "\n",
    "# Example Dutch medical note\n",
    "text = \"De patiënt heeft diabetes type 2 en kreeg een MRI-scan voor knieklachten.\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Print shape of the last hidden state\n",
    "print(outputs.last_hidden_state.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (MPNetModel) is not compatible with `.generate()`, as it doesn't have a language model head. Classes that support generation often end in one of these names: ['ForCausalLM', 'ForConditionalGeneration', 'ForSpeechSeq2Seq', 'ForVision2Seq'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Generate a summarized version\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m summary_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m summary \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(summary_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)  \u001b[38;5;66;03m# Should return a brief summary\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Y.vanMegen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Y.vanMegen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:2007\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \n\u001b[0;32m   1924\u001b[0m \u001b[38;5;124;03mGenerates sequences of token ids for models with a language modeling head.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;124;03m            - [`~generation.GenerateBeamEncoderDecoderOutput`]\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2006\u001b[0m \u001b[38;5;66;03m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2008\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Pull this out first, we only use it for stopping criteria\u001b[39;00m\n\u001b[0;32m   2009\u001b[0m assistant_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Y.vanMegen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1297\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcan_generate():\n\u001b[0;32m   1291\u001b[0m     terminations_with_generation_support \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForCausalLM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1293\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForConditionalGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1294\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForSpeechSeq2Seq\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1295\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForVision2Seq\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1296\u001b[0m     ]\n\u001b[1;32m-> 1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current model class (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is not compatible with `.generate()`, as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a language model head. Classes that support generation often end in one of these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1300\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterminations_with_generation_support\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1301\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: The current model class (MPNetModel) is not compatible with `.generate()`, as it doesn't have a language model head. Classes that support generation often end in one of these names: ['ForCausalLM', 'ForConditionalGeneration', 'ForSpeechSeq2Seq', 'ForVision2Seq']."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = \"FremyCompany/BioLORD-2023\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)  # Change from AutoModelForSeq2SeqLM to AutoModel\n",
    "\n",
    "# Example Dutch medical note\n",
    "text = \"De patiënt heeft diabetes type 2 en kreeg een MRI-scan voor knieklachten.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "# Generate a summarized version\n",
    "summary_ids = model.generate(**inputs)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(summary)  # Should return a brief summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: patiënt--------------------------dia-----------dia-----\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = \"facebook/mbart-large-cc25\"  # A multilingual text summarization model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "text = \"De patiënt heeft diabetes type 2 en kreeg een MRI-scan voor knieklachten.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate a summary\n",
    "summary_ids = model.generate(**inputs, max_length=50, num_beams=5)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.67\n",
      "No GIB detected.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"FremyCompany/BioLORD-2023\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to get sentence embedding\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Example medical report\n",
    "report = \"De patiënt werd opgenomen met bloederige diarree en melena, mogelijk als gevolg van een maagzweer.\"\n",
    "\n",
    "# Reference GIB-related phrases\n",
    "gib_references = [\n",
    "    \"Maagbloeding vastgesteld.\",\n",
    "    \"Melena waargenomen bij patiënt.\",\n",
    "    \"Patiënt heeft hematemesis gerapporteerd.\",\n",
    "    \"Er is een endoscopie uitgevoerd vanwege verdenking op GIB.\",\n",
    "]\n",
    "\n",
    "# Compute similarity\n",
    "report_embedding = get_embedding(report)\n",
    "reference_embeddings = torch.cat([get_embedding(ref) for ref in gib_references], dim=0)\n",
    "\n",
    "# Cosine similarity\n",
    "similarities = F.cosine_similarity(report_embedding, reference_embeddings)\n",
    "max_similarity = similarities.max().item()\n",
    "\n",
    "# Decide if GIB is present\n",
    "threshold = 0.75  # Adjust based on testing\n",
    "is_gib = max_similarity > threshold\n",
    "\n",
    "print(f\"Similarity score: {max_similarity:.2f}\")\n",
    "print(\"GIB detected!\" if is_gib else \"No GIB detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
