{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'a:/df_verslagen_cleaned.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_verslagen_clean \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma:/df_verslagen_cleaned.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df_verslagen_clean\n",
      "File \u001b[1;32mc:\\Users\\20193702\\OneDrive - TU Eindhoven\\JADS\\Jaar 2\\Thesis\\thesis\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\20193702\\OneDrive - TU Eindhoven\\JADS\\Jaar 2\\Thesis\\thesis\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\20193702\\OneDrive - TU Eindhoven\\JADS\\Jaar 2\\Thesis\\thesis\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\20193702\\OneDrive - TU Eindhoven\\JADS\\Jaar 2\\Thesis\\thesis\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\20193702\\OneDrive - TU Eindhoven\\JADS\\Jaar 2\\Thesis\\thesis\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'a:/df_verslagen_cleaned.csv'"
     ]
    }
   ],
   "source": [
    "df_verslagen_clean = pd.read_csv('a:/df_verslagen_cleaned.csv')\n",
    "df_verslagen_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>activity</th>\n",
       "      <th>verslagen_report_content</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>verslagen_report_content_cleaned</th>\n",
       "      <th>content_words</th>\n",
       "      <th>content_words_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6</td>\n",
       "      <td>Klinische Brief</td>\n",
       "      <td>Dhr. A.J. Dingemans, huisarts\\r\\n[STREETNAME] ...</td>\n",
       "      <td>2020-11-26 15:06:00</td>\n",
       "      <td>dhr aj dingemans huisarts streetname nr city d...</td>\n",
       "      <td>['dhr', 'aj', 'dingemans', 'huisarts', 'street...</td>\n",
       "      <td>['dhr', 'aj', 'dingemans', 'huisarts', 'street...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6</td>\n",
       "      <td>Consult, Kliniek: vervolgconsult</td>\n",
       "      <td>Samenvatting: \\nRectaal bloedverlies obv diver...</td>\n",
       "      <td>2020-11-26 09:53:00</td>\n",
       "      <td>samenvatting rectaal bloedverlies obv divertik...</td>\n",
       "      <td>['samenvatting', 'rectaal', 'bloedverlies', 'o...</td>\n",
       "      <td>['samenvatting', 'rectaal', 'bloedverlie', 'ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6</td>\n",
       "      <td>Poliklinische Brief</td>\n",
       "      <td>COLOSCOPIE\\r\\n\\r\\nBetreft\\r\\nMw. [INITIALS] [L...</td>\n",
       "      <td>2020-11-25 14:13:00</td>\n",
       "      <td>coloscopie betreft mw initials lastname adresg...</td>\n",
       "      <td>['coloscopie', 'betreft', 'mw', 'initials', 'l...</td>\n",
       "      <td>['coloscopie', 'betreffen', 'mw', 'initials', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6</td>\n",
       "      <td>Poliklinische Brief</td>\n",
       "      <td>GASTROSCOPIE\\r\\n\\r\\nBetreft\\r\\nMw. [INITIALS] ...</td>\n",
       "      <td>2020-11-25 13:48:00</td>\n",
       "      <td>gastroscopie betreft mw initials lastname adre...</td>\n",
       "      <td>['gastroscopie', 'betreft', 'mw', 'initials', ...</td>\n",
       "      <td>['gastroscopie', 'betreffen', 'mw', 'initials'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6</td>\n",
       "      <td>Consult, Kliniek: vervolgconsult</td>\n",
       "      <td>Samenvatting: \\nRectaal bloedverlies ; eenmali...</td>\n",
       "      <td>2020-11-25 08:47:00</td>\n",
       "      <td>samenvatting rectaal bloedverlies eenmalig hd ...</td>\n",
       "      <td>['samenvatting', 'rectaal', 'bloedverlies', 'e...</td>\n",
       "      <td>['samenvatting', 'rectaal', 'bloedverlie', 'ee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    case_id                          activity  \\\n",
       "0  046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6                   Klinische Brief   \n",
       "1  046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6  Consult, Kliniek: vervolgconsult   \n",
       "2  046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6               Poliklinische Brief   \n",
       "3  046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6               Poliklinische Brief   \n",
       "4  046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6  Consult, Kliniek: vervolgconsult   \n",
       "\n",
       "                            verslagen_report_content           timestamp  \\\n",
       "0  Dhr. A.J. Dingemans, huisarts\\r\\n[STREETNAME] ... 2020-11-26 15:06:00   \n",
       "1  Samenvatting: \\nRectaal bloedverlies obv diver... 2020-11-26 09:53:00   \n",
       "2  COLOSCOPIE\\r\\n\\r\\nBetreft\\r\\nMw. [INITIALS] [L... 2020-11-25 14:13:00   \n",
       "3  GASTROSCOPIE\\r\\n\\r\\nBetreft\\r\\nMw. [INITIALS] ... 2020-11-25 13:48:00   \n",
       "4  Samenvatting: \\nRectaal bloedverlies ; eenmali... 2020-11-25 08:47:00   \n",
       "\n",
       "                    verslagen_report_content_cleaned  \\\n",
       "0  dhr aj dingemans huisarts streetname nr city d...   \n",
       "1  samenvatting rectaal bloedverlies obv divertik...   \n",
       "2  coloscopie betreft mw initials lastname adresg...   \n",
       "3  gastroscopie betreft mw initials lastname adre...   \n",
       "4  samenvatting rectaal bloedverlies eenmalig hd ...   \n",
       "\n",
       "                                       content_words  \\\n",
       "0  ['dhr', 'aj', 'dingemans', 'huisarts', 'street...   \n",
       "1  ['samenvatting', 'rectaal', 'bloedverlies', 'o...   \n",
       "2  ['coloscopie', 'betreft', 'mw', 'initials', 'l...   \n",
       "3  ['gastroscopie', 'betreft', 'mw', 'initials', ...   \n",
       "4  ['samenvatting', 'rectaal', 'bloedverlies', 'e...   \n",
       "\n",
       "                            content_words_lemmatized  \n",
       "0  ['dhr', 'aj', 'dingemans', 'huisarts', 'street...  \n",
       "1  ['samenvatting', 'rectaal', 'bloedverlie', 'ob...  \n",
       "2  ['coloscopie', 'betreffen', 'mw', 'initials', ...  \n",
       "3  ['gastroscopie', 'betreffen', 'mw', 'initials'...  \n",
       "4  ['samenvatting', 'rectaal', 'bloedverlie', 'ee...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rename columns to match the event log\n",
    "df_verslagen_clean.rename(columns={'pseudo_id': 'case_id', 'verslagen_report_tags': 'activity', 'verslagen_report_start_date': 'timestamp'}, inplace=True)\n",
    "\n",
    "#make sure timestamp is in datetime format\n",
    "df_verslagen_clean['timestamp'] = pd.to_datetime(df_verslagen_clean['timestamp'])\n",
    "\n",
    "\n",
    "df_verslagen_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_verslagen_clean[['case_id', 'activity', 'timestamp']]  # Keep only the necessary columns\n",
    "df.to_csv('a:/df_verslagen_clean_event_log.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_verslagen_clean.to_csv('a:/df_verslagen_clean_event_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>activity</th>\n",
       "      <th>verslagen_report_content</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6</td>\n",
       "      <td>Consult, Polikliniek: eerste consult</td>\n",
       "      <td>Reden van komst / Verwijzing: \\nReden verwijzi...</td>\n",
       "      <td>2022-02-01 08:33:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6</td>\n",
       "      <td>Poliklinische Brief</td>\n",
       "      <td>Aan de weledelgeleerde vrouwe\\r\\ndrs. J.M.J. v...</td>\n",
       "      <td>2021-12-21 15:13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6</td>\n",
       "      <td>Consult, Polikliniek: vervolgconsult</td>\n",
       "      <td>Samenvatting: \\nVoorgeschiedenis\\ncholecystect...</td>\n",
       "      <td>2021-12-21 14:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6</td>\n",
       "      <td>Radiologieverslag, ECG</td>\n",
       "      <td>Indicatie:\\nZie dossier\\n\\nSR; 67/MIN; LINKER ...</td>\n",
       "      <td>2021-12-16 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6</td>\n",
       "      <td>Radiologieverslag, Thorax</td>\n",
       "      <td>Indicatie:\\nCRTH10X (CRTH10X): Röntgen borstka...</td>\n",
       "      <td>2021-12-07 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    case_id  \\\n",
       "0  046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6   \n",
       "1  046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6   \n",
       "2  046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6   \n",
       "3  046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6   \n",
       "4  046D1FFEBDD40E1665D0ABA6DD8FC9F8BC4351C6   \n",
       "\n",
       "                               activity  \\\n",
       "0  Consult, Polikliniek: eerste consult   \n",
       "1                   Poliklinische Brief   \n",
       "2  Consult, Polikliniek: vervolgconsult   \n",
       "3                Radiologieverslag, ECG   \n",
       "4             Radiologieverslag, Thorax   \n",
       "\n",
       "                            verslagen_report_content           timestamp  \n",
       "0  Reden van komst / Verwijzing: \\nReden verwijzi... 2022-02-01 08:33:00  \n",
       "1  Aan de weledelgeleerde vrouwe\\r\\ndrs. J.M.J. v... 2021-12-21 15:13:00  \n",
       "2  Samenvatting: \\nVoorgeschiedenis\\ncholecystect... 2021-12-21 14:50:00  \n",
       "3  Indicatie:\\nZie dossier\\n\\nSR; 67/MIN; LINKER ... 2021-12-16 00:00:00  \n",
       "4  Indicatie:\\nCRTH10X (CRTH10X): Röntgen borstka... 2021-12-07 00:00:00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alle_verslagen = pd.read_csv('a:/bloeding-met-patientenlijst-alle-soorten-verslagen/bloeding-met-patientenlijst-4-verslagen.csv')\n",
    "\n",
    "#rename columns to match the event log\n",
    "df_alle_verslagen.rename(columns={'pseudo_id': 'case_id', 'verslagen_report_tags': 'activity', 'verslagen_report_start_date': 'timestamp'}, inplace=True)\n",
    "\n",
    "#make sure timestamp is in datetime format\n",
    "df_alle_verslagen['timestamp'] = pd.to_datetime(df_alle_verslagen['timestamp'])\n",
    "\n",
    "\n",
    "df_alle_verslagen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_alle_verslagen[['case_id', 'activity', 'timestamp']]  # Keep only the necessary columns\n",
    "df_2.to_csv('a:/df_alle_verslagen_event_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nl-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/nl_core_news_sm-3.8.0/nl_core_news_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/12.8 MB 8.4 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.6/12.8 MB 6.6 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.2/12.8 MB 7.0 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.8/12.8 MB 7.2 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 7.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.1/12.8 MB 6.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 6.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 6.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 6.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 6.2 MB/s eta 0:00:00\n",
      "Installing collected packages: nl-core-news-sm\n",
      "Successfully installed nl-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('nl_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# pip install spacy\n",
    "# !python -m spacy download nl_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Verpleegkundige', 'Dr.'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "text = \"\"\"\n",
    "Dr. Jan Jansen stelde de diagnose. Verpleegkundige Lisa de Vries assisteerde.\n",
    "Pieter de Boer, de chirurg, voerde de procedure uit.\n",
    "\"\"\"\n",
    "\n",
    "# Regex for names with \"Dr.\" or \"Verpleegkundige\"\n",
    "pattern = r\"(Dr\\.|Verpleegkundige)\\s+[A-Z][a-z]+\\s+(?:[a-z]+\\s+)?[A-Z][a-z]+\"\n",
    "regex_matches = re.findall(pattern, text)\n",
    "\n",
    "# Extract all names with spaCy\n",
    "doc = nlp(text)\n",
    "nlp_matches = [ent.text for ent in doc.ents if ent.label_ == \"PER\"]\n",
    "\n",
    "# Combine results\n",
    "all_names = set(regex_matches + nlp_matches)\n",
    "\n",
    "print(all_names)  # Output: {'Dr. Jan Jansen', 'Verpleegkundige Lisa de Vries', 'Pieter de Boer'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Patiënt werd gezien door Dr. Jan Jansen en verwezen naar Verpleegkundige Lisa de Vries.\n",
      "Dr. Pieter de Boer voerde de operatie uit.\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_sm\")  # Load Dutch language model\n",
    "\n",
    "text = \"\"\"\n",
    "Patiënt werd gezien door Dr. Jan Jansen en verwezen naar Verpleegkundige Lisa de Vries.\n",
    "Dr. Pieter de Boer voerde de operatie uit.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "print(doc)\n",
    "\n",
    "# Extract all named entities labeled as PERSON (Dutch: \"PER\")\n",
    "names = [ent.text for ent in doc.ents if ent.label_ == \"PER\"]\n",
    "\n",
    "print(names)  # Output: ['Jan Jansen', 'Lisa de Vries', 'Pieter de Boer']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr. Jan', 'Jan Jansen', 'Lisa de', 'de Vries', 'Dr. Pieter', 'Pieter de', 'de Boer']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load Dutch language model\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "# Initialize pattern matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define a pattern for detecting names (Proper nouns)\n",
    "pattern = [{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}]  # Matches two consecutive proper nouns\n",
    "\n",
    "matcher.add(\"PERSON_NAMES\", [pattern])\n",
    "\n",
    "text = \"\"\"\n",
    "Patiënt werd gezien door Dr. Jan Jansen en verwezen naar Verpleegkundige Lisa de Vries.\n",
    "Dr. Pieter de Boer voerde de operatie uit.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Apply matcher to detect names\n",
    "matches = matcher(doc)\n",
    "names = [doc[start:end].text for match_id, start, end in matches]\n",
    "\n",
    "print(names)  # Output: ['Jan Jansen', 'Lisa de Vries', 'Pieter de Boer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dr.', 'Dr. Jan', 'Verpleegkundige', 'Lisa de', 'Pieter de', 'de Boer', 'Jan Jansen', 'de Vries'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "text = \"\"\"\n",
    "Dr. Jan Jansen stelde de diagnose. Verpleegkundige Lisa de Vries assisteerde.\n",
    "Pieter de Boer, de chirurg, voerde de procedure uit.\n",
    "\"\"\"\n",
    "\n",
    "# Regex pattern to catch \"Dr.\" or \"Verpleegkundige\"\n",
    "pattern = r\"(Dr\\.|Verpleegkundige)\\s+[A-Z][a-z]+\\s+(?:[a-z]+\\s+)?[A-Z][a-z]+\"\n",
    "regex_matches = re.findall(pattern, text)\n",
    "\n",
    "# Extract all names with spaCy's matcher\n",
    "doc = nlp(text)\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PERSON_NAMES\", [[{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}]])\n",
    "matches = [doc[start:end].text for _, start, end in matcher(doc)]\n",
    "\n",
    "# Combine results\n",
    "all_names = set(regex_matches + matches)\n",
    "\n",
    "print(all_names)  # Output: {'Dr. Jan Jansen', 'Verpleegkundige Lisa de Vries', 'Pieter de Boer'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr.', 'Verpleegkundige', 'Dr.']\n",
      "['Dr. Jan', 'Dr. Jan Jansen', 'Jan Jansen', 'Lisa de', 'Lisa de Vries', 'de Vries', 'Pieter de', 'Pieter de Boer', 'de Boer', 'Dr. Marieke', 'Dr. Marieke van', 'Marieke van', 'Marieke van Dijk', 'van Dijk']\n",
      "{'Dr.', 'van Dijk', 'Lisa de Vries', 'Dr. Marieke van', 'Marieke van', 'Dr. Marieke', 'Dr. Jan', 'Verpleegkundige', 'Lisa de', 'Pieter de', 'de Boer', 'Jan Jansen', 'Pieter de Boer', 'Marieke van Dijk', 'de Vries', 'Dr. Jan Jansen'}\n",
      "{'Dr. Marieke van', 'Lisa de Vries', 'Verpleegkundige', 'Pieter de Boer', 'Marieke van Dijk', 'Dr. Jan Jansen'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "text = \"\"\"\n",
    "Dr. Jan Jansen stelde de diagnose. Verpleegkundige Lisa de Vries assisteerde.\n",
    "Pieter de Boer, de chirurg, voerde de procedure uit.\n",
    "Dr. Marieke van Dijk onderzocht de patiënt.\n",
    "\"\"\"\n",
    "\n",
    "# 🔹 Updated regex to capture full Dutch names with prefixes\n",
    "pattern = r\"(Dr\\.|Verpleegkundige)\\s+[A-Z][a-z]+(?:\\s+(?:van|de|der)\\s+[A-Z][a-z]+)?\"  \n",
    "\n",
    "regex_matches = re.findall(pattern, text)\n",
    "print(regex_matches)\n",
    "\n",
    "# Process text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# 🔹 Matcher pattern for full Dutch names (supports prefixes)\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\", \"OP\": \"?\"}]  # Captures 2- or 3-part names\n",
    "matcher.add(\"PERSON_NAMES\", [pattern])\n",
    "\n",
    "# Extract full names using spaCy\n",
    "matches = [doc[start:end].text for _, start, end in matcher(doc)]\n",
    "print(matches)\n",
    "\n",
    "# 🔹 Clean and filter duplicate/split names\n",
    "all_names = set(regex_matches + matches)\n",
    "print(all_names)\n",
    "# filtered_names = {name for name in all_names if len(name.split()) > 1}  # Remove single-word fragments\n",
    "# print(filtered_names)\n",
    "\n",
    "# Filter out partial duplicates, keeping the longest match\n",
    "final_names = set()\n",
    "for name in all_names:\n",
    "    if not any(name in other and name != other for other in all_names):\n",
    "        final_names.add(name)\n",
    "\n",
    "print(final_names)  # Output: {'Dr. Jan Jansen', 'Verpleegkundige Lisa de Vries', 'Pieter de Boer', 'Marieke van Dijk'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install clinlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20193702\\OneDrive - TU Eindhoven\\JADS\\Jaar 2\\Thesis\\thesis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\20193702\\OneDrive - TU Eindhoven\\JADS\\Jaar 2\\Thesis\\thesis\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\20193702\\.cache\\huggingface\\hub\\models--UMCU--MedRoBERTa.nl_NegationDetection. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\20193702\\OneDrive - TU Eindhoven\\JADS\\Jaar 2\\Thesis\\thesis\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\20193702\\.cache\\huggingface\\hub\\models--UMCU--MedRoBERTa.nl_Experiencer. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import clinlp\n",
    "\n",
    "nlp = spacy.blank('clinlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"De patient krijgt 2x daags 500 mg paracetamol.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['De', 'patient', 'krijgt', '2', 'x', 'daags', '500', 'mg', 'paracetamol', '.']\n"
     ]
    }
   ],
   "source": [
    "print(list(token.text for token in doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<clinlp.sentencizer.Sentencizer at 0x21e87d3b590>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('clinlp_normalizer')\n",
    "nlp.add_pipe('clinlp_sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patient', 'krijgt', '2', 'x', 'daags', '500', 'mg', 'paracetamol', '.', 'de', 'patient', 'is', 'allergisch', 'voor', 'penicilline', '.']\n",
      "['Patiënt krijgt 2x daags 500 mg paracetamol.', 'De patiënt is allergisch voor penicilline.']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\n",
    "    \"Patiënt krijgt 2x daags 500 mg \"\n",
    "    \"paracetamol. De patiënt is allergisch \"\n",
    "    \"voor penicilline.\"\n",
    ")\n",
    "\n",
    "\n",
    "print([token.norm_ for token in doc])\n",
    "# ['patient', 'krijgt', '2', 'x', 'daags', '500', 'mg', 'paracetamol', '.', 'de', 'patient', 'is', 'allergisch', 'voor', 'penicilline', '.']\n",
    "\n",
    "print([str(sent) for sent in doc.sents])\n",
    "# ['Patiënt krijgt 2x daags 500 mg paracetamol.', 'De patiënt is allergisch voor penicilline.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clinlp_normalizer', 'clinlp_sentencizer']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clinlp.ie import Term\n",
    "\n",
    "terms = {\n",
    "    \"prematuriteit\": [\n",
    "        \"preterm\", \"<p3\", \"prematuriteit\", \"partus praematurus\"\n",
    "    ],\n",
    "    \"hypotensie\": [\n",
    "        \"hypotensie\", Term(\"bd verlaagd\", proximity=1)\n",
    "    ],\n",
    "    \"veneus_infarct\": [\n",
    "        \"veneus infarct\", Term(\"VI\", attr=\"TEXT\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "entity_matcher = nlp.add_pipe(\n",
    "    \"clinlp_rule_based_entity_matcher\", \n",
    "    config={\"attr\": \"NORM\", \"fuzzy\": 1}\n",
    ")\n",
    "\n",
    "entity_matcher.add_terms_from_dict(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preterme prematuriteit\n",
      "<p3 prematuriteit\n",
      "bd enigszins verlaagd hypotensie\n",
      "hypotensie hypotensie\n",
      "veneus infarkt veneus_infarct\n",
      "partus prematurus prematuriteit\n",
      "VI veneus_infarct\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Preterme neonaat (<p3) opgenomen, bd enigszins verlaagd, \"\n",
    "    \"familieanamnese vermeldt eveneens hypotensie bij moeder. \"\n",
    "    \"Thans geen aanwijzingen voor veneus infarkt wat ook geen \"\n",
    "    \"verklaring voor de partus prematurus is. Risico op VI \"\n",
    "    \"blijft aanwezig.\"\n",
    ")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.spans['ents']:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<clinlp.ie.qualifier.context_algorithm.ContextAlgorithm at 0x21e94001850>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"clinlp_context_algorithm\", config={\"phrase_matcher_attr\": \"NORM\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preterme {Qualifier(name='Experiencer', value='Patient', is_default=True, priority=0, prob=None), Qualifier(name='Temporality', value='Current', is_default=True, priority=0, prob=None), Qualifier(name='Presence', value='Present', is_default=True, priority=0, prob=None)}\n",
      "<p3 {Qualifier(name='Experiencer', value='Patient', is_default=True, priority=0, prob=None), Qualifier(name='Temporality', value='Current', is_default=True, priority=0, prob=None), Qualifier(name='Presence', value='Present', is_default=True, priority=0, prob=None)}\n",
      "bd enigszins verlaagd {Qualifier(name='Experiencer', value='Patient', is_default=True, priority=0, prob=None), Qualifier(name='Temporality', value='Current', is_default=True, priority=0, prob=None), Qualifier(name='Presence', value='Present', is_default=True, priority=0, prob=None)}\n",
      "hypotensie {Qualifier(name='Experiencer', value='Family', is_default=False, priority=2, prob=None), Qualifier(name='Temporality', value='Current', is_default=True, priority=0, prob=None), Qualifier(name='Presence', value='Present', is_default=True, priority=0, prob=None)}\n",
      "veneus infarkt {Qualifier(name='Presence', value='Absent', is_default=False, priority=2, prob=None), Qualifier(name='Experiencer', value='Patient', is_default=True, priority=0, prob=None), Qualifier(name='Temporality', value='Current', is_default=True, priority=0, prob=None)}\n",
      "partus prematurus {Qualifier(name='Experiencer', value='Patient', is_default=True, priority=0, prob=None), Qualifier(name='Temporality', value='Current', is_default=True, priority=0, prob=None), Qualifier(name='Presence', value='Present', is_default=True, priority=0, prob=None)}\n",
      "VI {Qualifier(name='Temporality', value='Future', is_default=False, priority=1, prob=None), Qualifier(name='Experiencer', value='Patient', is_default=True, priority=0, prob=None), Qualifier(name='Presence', value='Present', is_default=True, priority=0, prob=None)}\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.spans['ents']:\n",
    "    print(ent.text, ent._.qualifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from clinlp.data import InfoExtractionDataset\n",
    "\n",
    "# dataset = InfoExtractionDataset.from_json(\"data/mantra_gsc.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers spacy\n",
    "# !python -m spacy download nl_core_news_lg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at CLTL/MedRoBERTa.nl and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp_roberta: <transformers.pipelines.token_classification.TokenClassificationPipeline object at 0x0000021E06CB6650>\n",
      "doc: \n",
      "Dr. Jan de Vries is een cardioloog in het Erasmus MC. Verpleegkundige Sarah Jansen werkt op de IC-afdeling.\n",
      "Dr. Kees van Dijk is neurochirurg.\n",
      "\n",
      "ent: Jan de Vries\n",
      "ent: Erasmus MC\n",
      "ent: Sarah Jansen\n",
      "ent: Kees van Dijk\n",
      "word: Ċ\n",
      "word: Dr\n",
      "word: .\n",
      "word: ĠJan\n",
      "word: Ġde\n",
      "word: ĠVries\n",
      "word: Ġis\n",
      "word: Ġeen\n",
      "word: Ġcardioloog\n",
      "word: Ġin\n",
      "word: Ġhet\n",
      "word: ĠErasmus\n",
      "word: ĠMC\n",
      "word: .\n",
      "word: ĠVerpleegkundige\n",
      "word: ĠSara\n",
      "word: h\n",
      "word: ĠJan\n",
      "word: sen\n",
      "word: Ġwerkt\n",
      "word: Ġop\n",
      "word: Ġde\n",
      "word: ĠIC\n",
      "word: -\n",
      "word: afdeling\n",
      "word: .\n",
      "word: Ċ\n",
      "word: Dr\n",
      "word: .\n",
      "word: ĠK\n",
      "word: ees\n",
      "word: Ġvan\n",
      "word: ĠDijk\n",
      "word: Ġis\n",
      "word: Ġneurochirurg\n",
      "word: .\n",
      "word: Ċ\n",
      "r: {'naam': 'Jan de Vries', 'functie': 'ĠVerpleegkundige'}\n",
      "Naam: Jan de Vries, Functie: ĠVerpleegkundige\n",
      "r: {'naam': 'Sarah Jansen', 'functie': 'ĠVerpleegkundige'}\n",
      "Naam: Sarah Jansen, Functie: ĠVerpleegkundige\n",
      "r: {'naam': 'Kees van Dijk', 'functie': 'ĠVerpleegkundige'}\n",
      "Naam: Kees van Dijk, Functie: ĠVerpleegkundige\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# Laad het Nederlandse spaCy-model voor NER\n",
    "nlp = spacy.load(\"nl_core_news_lg\")\n",
    "\n",
    "# Laad MedRoBERTa.nl model en tokenizer\n",
    "# model_name = \"GroNLP/medroberta-base\"\n",
    "model_name = \"CLTL/MedRoBERTa.nl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Gebruik Hugging Face pipeline voor tokenclassificatie\n",
    "nlp_roberta = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "print('nlp_roberta:', nlp_roberta)\n",
    "\n",
    "def extract_medical_entities(text):\n",
    "    doc = nlp(text)\n",
    "    print('doc:', doc)\n",
    "    \n",
    "    professionals = []\n",
    "    \n",
    "    # Zoek naar persoonsnamen en functies met spaCy\n",
    "    for ent in doc.ents:\n",
    "        print('ent:', ent)\n",
    "        if ent.label_ in [\"PERSON\"]:  # Herkent namen\n",
    "            professionals.append({\"naam\": ent.text, \"functie\": None})\n",
    "\n",
    "    # Zoek naar medische functies met MedRoBERTa.nl\n",
    "    roberta_results = nlp_roberta(text)\n",
    "    for result in roberta_results:\n",
    "        # print('result:', result)\n",
    "        word = result['word']\n",
    "        print('word:', word)\n",
    "        label = result['entity']  # De entiteit die MedRoBERTa herkent\n",
    "        # print('label:', label)\n",
    "        \n",
    "        if \"arts\" in word.lower() or \"verpleegkundige\" in word.lower():\n",
    "            for person in professionals:\n",
    "                if person[\"functie\"] is None:\n",
    "                    person[\"functie\"] = word\n",
    "\n",
    "    return professionals\n",
    "\n",
    "# 🔍 **Test met voorbeeldtekst**\n",
    "tekst = \"\"\"\n",
    "Dr. Jan de Vries is een cardioloog in het Erasmus MC. Verpleegkundige Sarah Jansen werkt op de IC-afdeling.\n",
    "Dr. Kees van Dijk is neurochirurg.\n",
    "\"\"\"\n",
    "\n",
    "resultaten = extract_medical_entities(tekst)\n",
    "for r in resultaten:\n",
    "    print('r:', r)\n",
    "    print(f\"Naam: {r['naam']}, Functie: {r['functie']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at CLTL/MedRoBERTa.nl and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PatiÃ«nt', 'Ġkrijgt', 'Ġ2', 'x', 'Ġdaags', 'Ġ500', 'Ġmg', 'Ġparacetamol', '.', 'ĠDe', 'ĠpatiÃ«nt', 'Ġis', 'Ġallergisch', 'Ġvoor', 'Ġpenicilline', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"CLTL/MedRoBERTa.nl\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"CLTL/MedRoBERTa.nl\")\n",
    "\n",
    "# Tokenize the input text\n",
    "input_text = \"Patiënt krijgt 2x daags 500 mg paracetamol. De patiënt is allergisch voor penicilline.\"\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "\n",
    "# Normalize the tokens\n",
    "# normalized_tokens = [token.replace(\"Ġ\", \"\") for token in tokens]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vandaag vooral bezig geweest met de process mining en kijken wat er nu gebeurd. Er zitten best wat verschillen tussen beide datasets, maar ik denk dat de kleinere dataset over het algemeen goed genoeg is. bij de uitgebreide krijg je heel veel radiologieverslagen en die zijn denk ik niet zo relevant voor GIB. Cardiologie heb ik nog niet specifiek naar gekeken, maar kan wel een interessant iets zijn. \n",
    "\n",
    "Voor morgen wil ik verder met kijken of ik namen uit de tekst kan halen en hun functie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
